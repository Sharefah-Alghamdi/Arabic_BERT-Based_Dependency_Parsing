# Arabic_BERT-Based_Dependency_Parsing
This repository contains the code for the paper [“Parsing as Pretraining”](https://github.com/aghie/parsing-as-pretraining) by Vilares, David, et al., which proposes a novel method for improving the performance of natural language understanding models by using syntactic parsing as a pretraining task. 
The code also contains our updates to conduct our experiments for our article [**“Fine-Tuning BERT-Based Pre-Trained Models for Arabic Dependency Parsing”**](https://www.mdpi.com/2076-3417/13/7/4225) , where we used it to evaluate it on different Arabic treebanks with different Arabic BERT models.


# Colaboratory file
Finetune [AraBERTV02](https://huggingface.co/aubmindlab/bert-base-arabertv02) to perform sequence labeling dependency parsing on ArPoT and PADT treebanks. [HERE](https://colab.research.google.com/drive/1gAuqOabW1vm1mgdCNvbbaeluCP8CAnnK?usp=sharing)
